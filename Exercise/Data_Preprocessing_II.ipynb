{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3r1kIuk4Gs-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# # Install NLTK data jika belum diunduh\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4slaW6c67oZ",
        "outputId": "2dff509f-4df1-45e7-d4dc-e0bda7d478f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# # Install NLTK data jika belum diunduh\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# Contoh teks dalam Bahasa Indonesia\n",
        "text_id = \"Pada hari Minggu, kami berencana pergi ke taman bersama keluarga. Kami akan membawa makanan enak, seperti nasi goreng, ayam bakar, dan es krim. Semua orang sangat antusias untuk piknik ini.\"\n",
        "\n",
        "# Membersihkan teks dengan regex\n",
        "cleaned_text_id = re.sub(r'[^a-zA-Z]', ' ', text_id)  # Hanya menyisakan huruf a-z dan A-Z\n",
        "\n",
        "# Tokenisasi\n",
        "tokens_id = word_tokenize(cleaned_text_id.lower())  # Tokenisasi dan ubah ke huruf kecil\n",
        "\n",
        "# Stopwords\n",
        "stop_words_id = set(stopwords.words('indonesian'))\n",
        "filtered_tokens_id = [word for word in tokens_id if word not in stop_words_id]\n",
        "\n",
        "# Stemming\n",
        "factory = StemmerFactory()\n",
        "stemmer_id = factory.create_stemmer()\n",
        "stemmed_tokens_id = [stemmer_id.stem(word) for word in filtered_tokens_id]\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Teks Asli (Bahasa Indonesia):\", text_id)\n",
        "print(\"Teks yang Telah Dicleanse: \", cleaned_text_id)\n",
        "print(\"Tokenisasi: \", tokens_id)\n",
        "print(\"Stopwords: \", filtered_tokens_id)\n",
        "print(\"Stemming: \", stemmed_tokens_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1BTjY_C5RbC",
        "outputId": "955cee25-a9e7-407b-e22d-235786a7f1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teks Asli (Bahasa Indonesia): Pada hari Minggu, kami berencana pergi ke taman bersama keluarga. Kami akan membawa makanan enak, seperti nasi goreng, ayam bakar, dan es krim. Semua orang sangat antusias untuk piknik ini.\n",
            "Teks yang Telah Dicleanse:  Pada hari Minggu  kami berencana pergi ke taman bersama keluarga  Kami akan membawa makanan enak  seperti nasi goreng  ayam bakar  dan es krim  Semua orang sangat antusias untuk piknik ini \n",
            "Tokenisasi:  ['pada', 'hari', 'minggu', 'kami', 'berencana', 'pergi', 'ke', 'taman', 'bersama', 'keluarga', 'kami', 'akan', 'membawa', 'makanan', 'enak', 'seperti', 'nasi', 'goreng', 'ayam', 'bakar', 'dan', 'es', 'krim', 'semua', 'orang', 'sangat', 'antusias', 'untuk', 'piknik', 'ini']\n",
            "Stopwords:  ['minggu', 'berencana', 'pergi', 'taman', 'keluarga', 'membawa', 'makanan', 'enak', 'nasi', 'goreng', 'ayam', 'bakar', 'es', 'krim', 'orang', 'antusias', 'piknik']\n",
            "Stemming:  ['minggu', 'rencana', 'pergi', 'taman', 'keluarga', 'bawa', 'makan', 'enak', 'nasi', 'goreng', 'ayam', 'bakar', 'es', 'krim', 'orang', 'antusias', 'piknik']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# # Install NLTK data jika belum diunduh\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# Contoh teks dalam Bahasa Inggris\n",
        "text_en = \"On Sunday, we are planning to go to the park with our family. We will bring delicious food such as fried rice, grilled chicken, and ice cream. Everyone is very excited for this picnic.\"\n",
        "\n",
        "# Membersihkan teks dengan regex\n",
        "cleaned_text_en = re.sub(r'[^a-zA-Z]', ' ', text_en)  # Hanya menyisakan huruf a-z dan A-Z\n",
        "\n",
        "# Tokenization\n",
        "tokens_en = word_tokenize(cleaned_text_en.lower())  # Tokenisasi dan ubah ke huruf kecil\n",
        "\n",
        "# Stopwords\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "filtered_tokens_en = [word for word in tokens_en if word not in stop_words_en]\n",
        "\n",
        "# Stemming\n",
        "stemmer_en = PorterStemmer()\n",
        "stemmed_tokens_en = [stemmer_en.stem(word) for word in filtered_tokens_en]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer_en = WordNetLemmatizer()\n",
        "lemmatized_tokens_en = [lemmatizer_en.lemmatize(word) for word in filtered_tokens_en]\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Original Text (English):\", text_en)\n",
        "print(\"Cleansed Text: \", cleaned_text_en)\n",
        "print(\"Tokenization: \", tokens_en)\n",
        "print(\"Stopwords Removed: \", filtered_tokens_en)\n",
        "print(\"Stemming: \", stemmed_tokens_en)\n",
        "print(\"Lemmatization: \", lemmatized_tokens_en)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M20BUr696kv_",
        "outputId": "c3bed3d3-8422-47c5-f267-dbdbbe0ddd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text (English): On Sunday, we are planning to go to the park with our family. We will bring delicious food such as fried rice, grilled chicken, and ice cream. Everyone is very excited for this picnic.\n",
            "Cleansed Text:  On Sunday  we are planning to go to the park with our family  We will bring delicious food such as fried rice  grilled chicken  and ice cream  Everyone is very excited for this picnic \n",
            "Tokenization:  ['on', 'sunday', 'we', 'are', 'planning', 'to', 'go', 'to', 'the', 'park', 'with', 'our', 'family', 'we', 'will', 'bring', 'delicious', 'food', 'such', 'as', 'fried', 'rice', 'grilled', 'chicken', 'and', 'ice', 'cream', 'everyone', 'is', 'very', 'excited', 'for', 'this', 'picnic']\n",
            "Stopwords Removed:  ['sunday', 'planning', 'go', 'park', 'family', 'bring', 'delicious', 'food', 'fried', 'rice', 'grilled', 'chicken', 'ice', 'cream', 'everyone', 'excited', 'picnic']\n",
            "Stemming:  ['sunday', 'plan', 'go', 'park', 'famili', 'bring', 'delici', 'food', 'fri', 'rice', 'grill', 'chicken', 'ice', 'cream', 'everyon', 'excit', 'picnic']\n",
            "Lemmatization:  ['sunday', 'planning', 'go', 'park', 'family', 'bring', 'delicious', 'food', 'fried', 'rice', 'grilled', 'chicken', 'ice', 'cream', 'everyone', 'excited', 'picnic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_en = \" On Sunday, we are planning to go to the park with our family. We will bring delicious food such as fried rice, grilled chicken, and ice cream.\"\n",
        "\n",
        "# Cleansing dengan regex\n",
        "cleaned_text_en = re.sub(r'[^a-zA-Z]', ' ', text_en)  # Hanya menyisakan huruf a-z dan A-Z\n",
        "cleaned_text_en = re.sub(r'\\s+', ' ', cleaned_text_en)  # Mengganti spasi berlebih dengan satu spasi\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Original Text: \", text_en)\n",
        "print(\"Cleansed Text: \", cleaned_text_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOgtgAmJIDVB",
        "outputId": "84718a43-9b29-4978-80bf-fcf786126e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:   On Sunday, we are planning to go to the park with our family. We will bring delicious food such as fried rice, grilled chicken, and ice cream.\n",
            "Cleansed Text:   On Sunday we are planning to go to the park with our family We will bring delicious food such as fried rice grilled chicken and ice cream \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens_en = word_tokenize(cleaned_text_en.lower())  # Tokenisasi dan ubah ke huruf kecil\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Tokenization: \", tokens_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UniIziLcKKvV",
        "outputId": "816ce551-fea5-46e1-c8c2-c6e020e4d9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:  ['on', 'sunday', 'we', 'are', 'planning', 'to', 'go', 'to', 'the', 'park', 'with', 'our', 'family', 'we', 'will', 'bring', 'delicious', 'food', 'such', 'as', 'fried', 'rice', 'grilled', 'chicken', 'and', 'ice', 'cream']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "filtered_tokens_en = [word for word in tokens_en if word not in stop_words_en]\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Stopwords Removed: \", filtered_tokens_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09ykSRK4KN9x",
        "outputId": "e1945f35-df55-42d4-d39b-c31a9a0c90a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords Removed:  ['sunday', 'planning', 'go', 'park', 'family', 'bring', 'delicious', 'food', 'fried', 'rice', 'grilled', 'chicken', 'ice', 'cream']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer_en = PorterStemmer()\n",
        "stemmed_tokens_en = [stemmer_en.stem(word) for word in filtered_tokens_en]\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Stemming: \", stemmed_tokens_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Z1x5IRKOy3",
        "outputId": "f2043ca9-135b-49e0-e15b-9ca3b7882607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming:  ['sunday', 'plan', 'go', 'park', 'famili', 'bring', 'delici', 'food', 'fri', 'rice', 'grill', 'chicken', 'ice', 'cream']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer_en = WordNetLemmatizer()\n",
        "lemmatized_tokens_en = [lemmatizer_en.lemmatize(word) for word in filtered_tokens_en]\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Stemming: \", stemmed_tokens_en)\n",
        "print(\"Lemmatization: \", lemmatized_tokens_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1B8PaUoKQk1",
        "outputId": "4e144513-e496-47f1-8573-523c747ebda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming:  ['sunday', 'plan', 'go', 'park', 'famili', 'bring', 'delici', 'food', 'fri', 'rice', 'grill', 'chicken', 'ice', 'cream']\n",
            "Lemmatization:  ['sunday', 'planning', 'go', 'park', 'family', 'bring', 'delicious', 'food', 'fried', 'rice', 'grilled', 'chicken', 'ice', 'cream']\n"
          ]
        }
      ]
    }
  ]
}